{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 16, kernel_size=3)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "data_path = '../data-unversioned/p1ch7/'\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True)\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "for img, label in cifar10\n",
    "if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "for img, label in cifar10_val\n",
    "if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "to_tensor = transforms.ToTensor()\n",
    "img_tensor = to_tensor(img)\n",
    "output = conv(img_tensor.unsqueeze(0))\n",
    "img_tensor.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "img_tensor = to_tensor(img)\n",
    "output = conv(img_tensor.unsqueeze(0))\n",
    "img_tensor.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    conv.bias.zero_()\n",
    "with torch.no_grad():\n",
    "    conv.weight.fill_(1.0 / 9.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_tensor = to_tensor(img)\n",
    "output = conv(img_tensor.unsqueeze(0))\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "with torch.no_grad():\n",
    "    conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n",
    "                                    [-1.0, 0.0, 1.0],\n",
    "                                    [-1.0, 0.0, 1.0]])\n",
    "    conv.bias.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(2)\n",
    "img_tensor = to_tensor(img)\n",
    "output = pool(img_tensor.unsqueeze(0))\n",
    "img_tensor.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "nn.Tanh(),\n",
    "nn.MaxPool2d(2),\n",
    "nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "nn.Tanh(),\n",
    "nn.MaxPool2d(2),\n",
    "# ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "nn.Tanh(),\n",
    "nn.MaxPool2d(2),\n",
    "nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "nn.Tanh(),\n",
    "nn.MaxPool2d(2),\n",
    "# ...\n",
    "nn.Linear(8 * 8 * 8, 32),\n",
    "nn.Tanh(),\n",
    "nn.Linear(32, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x8 and 512x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m img_tensor \u001b[38;5;241m=\u001b[39m to_tensor(img)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x8 and 512x32)"
     ]
    }
   ],
   "source": [
    "img_tensor = to_tensor(img)\n",
    "model(img_tensor.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1827,  0.1720]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "img_tensor = to_tensor(img)\n",
    "model(img_tensor.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "            datetime.datetime.now(), epoch,\n",
    "            loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch 1, Training loss: 2.2342444289370875\n",
      "Epoch 2, Training loss: 1.9671153305741527\n",
      "Epoch 3, Training loss: 1.8050441938592954\n",
      "Epoch 4, Training loss: 1.6571836424300739\n",
      "Epoch 5, Training loss: 1.5359868430115682\n",
      "Epoch 6, Training loss: 1.4481024454011941\n",
      "Epoch 7, Training loss: 1.3815502913864068\n",
      "Epoch 8, Training loss: 1.324568565925369\n",
      "Epoch 9, Training loss: 1.2751278681370912\n",
      "Epoch 10, Training loss: 1.231306984982527\n",
      "Epoch 11, Training loss: 1.1921045279411404\n",
      "Epoch 12, Training loss: 1.152377549720847\n",
      "Epoch 13, Training loss: 1.120443366220235\n",
      "Epoch 14, Training loss: 1.0860549040005336\n",
      "Epoch 15, Training loss: 1.0570426439232838\n",
      "Epoch 16, Training loss: 1.0266056426650727\n",
      "Epoch 17, Training loss: 0.999099355295796\n",
      "Epoch 18, Training loss: 0.9716730805309227\n",
      "Epoch 19, Training loss: 0.9467499282811304\n",
      "Epoch 20, Training loss: 0.9229650077460062\n",
      "Epoch 21, Training loss: 0.8982429008197297\n",
      "Epoch 22, Training loss: 0.8738412805225538\n",
      "Epoch 23, Training loss: 0.8514846289325553\n",
      "Epoch 24, Training loss: 0.8289624039093246\n",
      "Epoch 25, Training loss: 0.8083741710237835\n",
      "Epoch 26, Training loss: 0.7869693603357086\n",
      "Epoch 27, Training loss: 0.7674575097039532\n",
      "Epoch 28, Training loss: 0.7457109749545832\n",
      "Epoch 29, Training loss: 0.7240155232531945\n",
      "Epoch 30, Training loss: 0.7056668344361093\n",
      "Epoch 31, Training loss: 0.6858445676162724\n",
      "Epoch 32, Training loss: 0.6671265341208109\n",
      "Epoch 33, Training loss: 0.6474616911900622\n",
      "Epoch 34, Training loss: 0.6267267684726154\n",
      "Epoch 35, Training loss: 0.6098072215952837\n",
      "Epoch 36, Training loss: 0.5889702879483133\n",
      "Epoch 37, Training loss: 0.5746596805808489\n",
      "Epoch 38, Training loss: 0.5550324319459289\n",
      "Epoch 39, Training loss: 0.5333112793810227\n",
      "Epoch 40, Training loss: 0.5156742154103716\n",
      "Epoch 41, Training loss: 0.49696497565797526\n",
      "Epoch 42, Training loss: 0.4788715511827213\n",
      "Epoch 43, Training loss: 0.4603404270870911\n",
      "Epoch 44, Training loss: 0.44535629304549884\n",
      "Epoch 45, Training loss: 0.42457066586865183\n",
      "Epoch 46, Training loss: 0.4059735989517263\n",
      "Epoch 47, Training loss: 0.3934488579668962\n",
      "Epoch 48, Training loss: 0.37277127089707746\n",
      "Epoch 49, Training loss: 0.35564572216414125\n",
      "Epoch 50, Training loss: 0.33833562846645676\n",
      "Epoch 51, Training loss: 0.3211289272331597\n",
      "Epoch 52, Training loss: 0.3051521000940629\n",
      "Epoch 53, Training loss: 0.290485223293152\n",
      "Epoch 54, Training loss: 0.27635855141007687\n",
      "Epoch 55, Training loss: 0.2598611534003864\n",
      "Epoch 56, Training loss: 0.24825949530543573\n",
      "Epoch 57, Training loss: 0.22915144252312153\n",
      "Epoch 58, Training loss: 0.21773368366005474\n",
      "Epoch 59, Training loss: 0.20840018833784954\n",
      "Epoch 60, Training loss: 0.19171022035924676\n",
      "Epoch 61, Training loss: 0.17769537848965897\n",
      "Epoch 62, Training loss: 0.16801736402370587\n",
      "Epoch 63, Training loss: 0.15204132349013597\n",
      "Epoch 64, Training loss: 0.14095817502978666\n",
      "Epoch 65, Training loss: 0.1310496088069723\n",
      "Epoch 66, Training loss: 0.12499057025174655\n",
      "Epoch 67, Training loss: 0.10732782317701813\n",
      "Epoch 68, Training loss: 0.10623049702914551\n",
      "Epoch 69, Training loss: 0.09079531921535883\n",
      "Epoch 70, Training loss: 0.08441741834811466\n",
      "Epoch 71, Training loss: 0.07653489177022367\n",
      "Epoch 72, Training loss: 0.060937268346252724\n",
      "Epoch 73, Training loss: 0.05637224839495309\n",
      "Epoch 74, Training loss: 0.047572862375956364\n",
      "Epoch 75, Training loss: 0.041341546584692454\n",
      "Epoch 76, Training loss: 0.03333793202762866\n",
      "Epoch 77, Training loss: 0.029945893950946153\n",
      "Epoch 78, Training loss: 0.025043130684953632\n",
      "Epoch 79, Training loss: 0.02201038084404967\n",
      "Epoch 80, Training loss: 0.020932956720771424\n",
      "Epoch 81, Training loss: 0.01850180916459588\n",
      "Epoch 82, Training loss: 0.01637907171189604\n",
      "Epoch 83, Training loss: 0.014092179857280172\n",
      "Epoch 84, Training loss: 0.013062369090307247\n",
      "Epoch 85, Training loss: 0.011905208706696186\n",
      "Epoch 86, Training loss: 0.01066316073269719\n",
      "Epoch 87, Training loss: 0.010083734558935723\n",
      "Epoch 88, Training loss: 0.009562507164283463\n",
      "Epoch 89, Training loss: 0.008784539916593096\n",
      "Epoch 90, Training loss: 0.007904910823077801\n",
      "Epoch 91, Training loss: 0.007520975152333088\n",
      "Epoch 92, Training loss: 0.007322067189116098\n",
      "Epoch 93, Training loss: 0.00715806125990732\n",
      "Epoch 94, Training loss: 0.0066394261903274816\n",
      "Epoch 95, Training loss: 0.0059142193877283495\n",
      "Epoch 96, Training loss: 0.005553709636347087\n",
      "Epoch 97, Training loss: 0.005549009860781453\n",
      "Epoch 98, Training loss: 0.00527413515195422\n",
      "Epoch 99, Training loss: 0.004995690613134723\n",
      "Epoch 100, Training loss: 0.005704269460414338\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Definir la arquitectura del modelo\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  # Entrada con 3 canales (RGB), 16 filtros\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 salidas para 10 clases\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 32 * 8 * 8)  # Aplanar el tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Definir el bucle de entrenamiento\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch}, Training loss: {loss_train/len(train_loader)}')\n",
    "\n",
    "# Transformaciones para el conjunto de datos\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Cargar el conjunto de datos CIFAR-10\n",
    "cifar2 = CIFAR10(root='data', train=True, transform=transform, download=True)\n",
    "\n",
    "# Crear el DataLoader\n",
    "train_loader = DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "# Inicializar el modelo, el optimizador y la función de pérdida\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Ejecutar el bucle de entrenamiento\n",
    "training_loop(\n",
    "    n_epochs=100,\n",
    "    optimizer=optimizer,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=train_loader,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Accuracy train: 0.10\n",
      "Accuracy val: 0.10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Definir la arquitectura del modelo (supongo que ya tienes un modelo llamado Net)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 32 * 8 * 8)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Transformaciones para convertir las imágenes a tensores\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Cargar los conjuntos de datos con la transformación\n",
    "cifar2 = CIFAR10(root='data', train=True, transform=transform, download=True)\n",
    "cifar2_val = CIFAR10(root='data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Crear los DataLoaders\n",
    "train_loader = DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "val_loader = DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "# Función para validar el modelo\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name, correct / total))\n",
    "\n",
    "# Inicializar el modelo\n",
    "model = Net()\n",
    "\n",
    "# Ejecutar la validación\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), data_path + 'birds_vs_airplanes.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_10520\\398628253.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load(data_path + 'birds_vs_airplanes.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net()\n",
    "loaded_model.load_state_dict(torch.load(data_path + 'birds_vs_airplanes.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(datetime.datetime.now(), epoch,loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-15 15:53:00.239850 Epoch 1, Training loss 2.239286927615895\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)\n\u001b[0;32m      5\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[49], line 5\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(n_epochs, optimizer, model, loss_fn, train_loader)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      4\u001b[0m     loss_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      6\u001b[0m         imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m      7\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\cifar.py:119\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    116\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\functional.py:172\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    171\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[1;32m--> 172\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_num_channels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "shuffle=True)\n",
    "model = Net().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "training_loop(\n",
    "n_epochs = 100,\n",
    "optimizer = optimizer,\n",
    "model = model,\n",
    "loss_fn = loss_fn,\n",
    "train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
